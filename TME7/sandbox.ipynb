{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode\n",
    "\n",
    "from torch import nn\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# collect_fn for dataloader\n",
    "\n",
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_n, vocab_n):\n",
    "        super().__init__()\n",
    "        self.whh = nn.Linear(in_features=hidden_n, out_features=hidden_n, bias=False)\n",
    "        self.wxh = nn.Linear(in_features=vocab_n, out_features=hidden_n, bias=True)\n",
    "        self.why = nn.Linear(in_features=hidden_n, out_features=vocab_n, bias=True)\n",
    "        \n",
    "    #def forward(self, x):\n",
    "    #    # update the hidden state\n",
    "    #    self.h = torch.tanh(torch.mm(self.W_hh, self.h) + torch.mm(self.W_xh, x) + bh)\n",
    "    #    # compute the output vector\n",
    "    #    y = torch.mm(self.W_hy, self.h) + by\n",
    "    #    return y\n",
    "    def forward(self, h, x):\n",
    "        # update the hidden state\n",
    "        whhh = self.whh(h)\n",
    "        wxhx = self.wxh(x)\n",
    "        h_ = torch.tanh(whhh + wxhx) \n",
    "        # compute the output vector\n",
    "        y = self.why(h_)\n",
    "        return h_, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = []\n",
    "for path in glob.glob(\"data/names/*.txt\"):\n",
    "    classname = path.split('/')[-1].split('.')[0]\n",
    "    abspath = os.path.abspath(path)\n",
    "    names = pd.read_csv(abspath, header=None)\n",
    "    names['label'] = [classname] * names.shape[0]\n",
    "    dataframes.append(names)\n",
    "data = pd.concat(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['X'] = data[0].apply(str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set([char for char in ''.join(data.X.values.tolist())])\n",
    "SEQUENCE_END = '_'\n",
    "NULL_VALUE = '0'\n",
    "vocab.add(SEQUENCE_END)\n",
    "vocab.add(NULL_VALUE)\n",
    "\n",
    "vocab2int = dict(zip(vocab,np.arange(len(vocab))))\n",
    "\n",
    "appended = data.X + '_'\n",
    "converted = appended.apply(lambda name : [vocab2int[c] for c in name])\n",
    "data['converted'] = converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = set(data.label)\n",
    "classes_n = len(classes)\n",
    "class2int = dict(zip(classes, np.arange(len(classes))))\n",
    "\n",
    "def onehot_label(dico, value):\n",
    "    y_onehot = np.zeros((len(dico),1))\n",
    "    y_onehot[class2int[value]] = 1\n",
    "    return y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "vocab_n = len(vocab)\n",
    "hidden_n = 3\n",
    "rnn = RNN(hidden_n=hidden_n, vocab_n=vocab_n).to(device=device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters())\n",
    "\n",
    "# TRAINING\n",
    "for index, (name, label) in data[['converted', 'label']].iterrows():\n",
    "    batch_size = 1\n",
    "    y = onehot_label(class2int, label)\n",
    "    \n",
    "    x_onehot = torch.FloatTensor(len(name), vocab_n).to(device=device)\n",
    "    x = torch.from_numpy(np.array(name)).to(device=device)\n",
    "\n",
    "    x_onehot.zero_()\n",
    "    x_onehot.scatter_(1, x.view(-1,1), 1)\n",
    "    \n",
    "    output = []\n",
    "    hx = torch.zeros(hidden_n).to(device=device).float()\n",
    "    for i in range(len(name)-1):\n",
    "        hx,y_pred = rnn(hx, x_onehot[i,:])\n",
    "        output.append(y_pred)\n",
    "        \n",
    "    # Decoding\n",
    "    # loss and gradient update from hx & y\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    for i in reversed(range(1,len(name))):\n",
    "        y_pred = output[i-1].view(batch_size,-1)\n",
    "        y_char = x_onehot[i, :].argmax().unsqueeze(0)\n",
    "    \n",
    "        #import pdb; pdb.set_trace()\n",
    "        loss = criterion(y_pred, y_char.long())\n",
    "        not_last_char = i > 0\n",
    "        loss.backward(retain_graph=not_last_char)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    print(rnn(torch.zeros(hidden_n).to(device=device).float(),x_onehot[2,:])[1].argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
